import { useCallback, useRef, useState, useEffect } from 'react';
import { useHexaStore, VoiceState } from '../store/hexaStore';
import { isVoiceDisabledNow, silenceAudioEverywhere } from '@/lib/voiceDisableGuard';
import { getSessionSend, isRealtimeReady } from '@/lib/voiceSessionUtils';
import { injectExternalContext } from '@/lib/externalContext';
import { useExternalDataStore } from '@/store/externalDataStore';
import { initializeOpenAIAgentOnce } from './voiceAgentInitializer';
import { sessionIsolation } from '@/lib/sessionIsolation';

interface VoiceConnectionServiceOptions {
  setVoiceState: (state: VoiceState) => void;
  onError?: (error: string) => void;
  onResponse?: (text: string) => void;
  onTranscript?: (text: string) => void;
  initializeOpenAIAgentFromWorker: () => Promise<void>;
  initializeOpenAIAgent: (sessionData: any) => Promise<any>;
  openaiAgentRef: React.MutableRefObject<any>;
  setSessionInfo: (info: any) => void;
  setResponse: (text: string) => void;
  setTranscript: (text: string) => void;
  startSpeaking?: () => void;
  stopSpeaking?: () => void;
  setSpeechIntensity?: (intensity: number) => void;
}

export const useVoiceConnectionService = ({
  setVoiceState,
  onError,
  onResponse,
  onTranscript,
  initializeOpenAIAgentFromWorker,
  initializeOpenAIAgent,
  openaiAgentRef,
  setSessionInfo,
  setResponse,
  setTranscript,
  startSpeaking,
  stopSpeaking,
  setSpeechIntensity
}: VoiceConnectionServiceOptions) => {
  
  const { setInitializationState, setInitializationProgress } = useHexaStore();
  
  // Set up global function to receive response text from session events
  useEffect(() => {
    (window as any).__hexaSetResponse = (text: any) => {
      console.log('üåê Global response received:', text);
      
      // Only process string responses, ignore arrays or objects
      if (typeof text === 'string' && text.trim()) {
        setResponse(text);
        onResponse?.(text);
      } else {
        console.log('‚ö†Ô∏è Ignoring non-string response:', typeof text, text);
      }
    };

    (window as any).__hexaSetTranscript = (text: any) => {
      console.log('üåê Global transcript received:', text);
      
      // Only process string responses, ignore arrays or objects
      if (typeof text === 'string' && text.trim()) {
        setTranscript(text);
        onTranscript?.(text);
      } else {
        console.log('‚ö†Ô∏è Ignoring non-string transcript:', typeof text, text);
      }
    };
    
    return () => {
      delete (window as any).__hexaSetResponse;
      delete (window as any).__hexaSetTranscript;
    };
  }, [setResponse, onResponse, setTranscript, onTranscript]);
  
  // Store external data for voice agent context
  const [externalData, setExternalData] = useState<{
    image?: string;
    text?: string;
    prompt?: string;
    type?: string;
  } | null>(null);
  
  // Track last processed data to prevent duplicates
  const lastProcessedDataRef = useRef<string | null>(null);
  const lastSessionIdRef = useRef<string | null>(null);
  const sessionRefreshInFlightRef = useRef(false);
  const pendingSessionInfoRef = useRef<any>(null);
  
  // Helper function to check if data is duplicate
  const isDuplicateData = (data: any): boolean => {
    // Only check for duplicates based on the actual text content
    const textContent = data.text || '';
    
    if (lastProcessedDataRef.current === textContent) {
      console.log('üîç Duplicate detected based on text content:', textContent.substring(0, 100) + '...');
      return true;
    }
    
    lastProcessedDataRef.current = textContent;
    console.log('üîç New data accepted, text content:', textContent.substring(0, 100) + '...');
    return false;
  };
  
  // Synthetic flapping loop to guarantee mouth motion when speaking events arrive
  const flapRafRef = useRef<number | null>(null);
  const startSyntheticFlap = () => {
    if (flapRafRef.current !== null) return;
    const loop = () => {
      // Simple on/off flap between 0.35 and ~0.60 openness
      const t = performance.now() / 1000;
      const value = 0.35 + Math.max(0, Math.sin(t * 6.0)) * 0.25;
      setSpeechIntensity?.(value);
      flapRafRef.current = requestAnimationFrame(loop);
    };
    flapRafRef.current = requestAnimationFrame(loop);
  };
  const stopSyntheticFlap = () => {
    if (flapRafRef.current !== null) {
      cancelAnimationFrame(flapRafRef.current);
      flapRafRef.current = null;
    }
  };

  // Connect using SSE for receiving messages
  const handleSessionInfo = useCallback(
    async (d: any) => {
      console.log('Session info received, updating OpenAI Agent...');
      console.log('dY"? Received session_info with sessionId:', d.sessionId);
      setInitializationProgress(80);
      setSessionInfo(d);

      const newSessionId = d.sessionId ?? null;
      if (newSessionId) {
        localStorage.setItem('voiceSessionId', newSessionId);
        console.log('dY"? Stored voice session ID for external data sync:', newSessionId);
        console.log('dY"? localStorage now contains:', localStorage.getItem('voiceSessionId'));
      }

      const voiceDisabled = useHexaStore.getState().isVoiceDisabled;
      const existingSession = openaiAgentRef.current;
      const existingSessionId = lastSessionIdRef.current;
      const rtcState = existingSession?._pc?.connectionState;
      const transportReady = isRealtimeReady(existingSession);
      const sessionChanged =
        !!existingSession && !!newSessionId && !!existingSessionId && newSessionId !== existingSessionId;
      const rtcLost =
        !!existingSession &&
        (!transportReady || (rtcState && rtcState !== 'connected' && rtcState !== 'completed'));

      if (voiceDisabled && existingSession && !sessionChanged && !rtcLost) {
        console.log('üîá Voice disabled: preserving session without refresh');
        pendingSessionInfoRef.current = d;
        return;
      }

      if (!existingSession || sessionChanged || rtcLost) {
        if (sessionRefreshInFlightRef.current) {
          console.log('dY"? Reinitialization already in progress; skipping duplicate session_info handling');
          if (newSessionId) {
            lastSessionIdRef.current = newSessionId;
          }
          return;
        }

        // Additional guard: check global init mutex
        if ((window as any).__realtimeInitInFlight) {
          console.log('‚è≥ Global realtime init already in flight; ignoring duplicate session_info');
          if (newSessionId) {
            lastSessionIdRef.current = newSessionId;
          }
          return;
        }

        sessionRefreshInFlightRef.current = true;
        try {
          if (existingSession) {
            console.warn('dY"? Closing stale OpenAI Realtime session before reinitializing', {
              existingSessionId,
              newSessionId,
              rtcState,
              transportReady,
            });
            try {
              existingSession.close?.();
            } catch (error) {
              console.error('Failed to close existing session:', error);
            }
            openaiAgentRef.current = null;
            try {
              (window as any).activeSession = null;
            } catch (clearError) {
              console.warn('Failed to clear active session reference:', clearError);
            }
            stopSyntheticFlap();
            setSpeechIntensity?.(0);
            stopSpeaking?.();
            setVoiceState('idle');
          }

          const session = await initializeOpenAIAgentOnce(d, {
            setVoiceState,
            onError,
            startSpeaking,
            stopSpeaking,
            setSpeechIntensity,
          });

          if (session) {
            openaiAgentRef.current = session;
            lastSessionIdRef.current = newSessionId ?? null;
            if (externalData) {
              console.log('dY"? Passing external data to refreshed agent:', externalData);
            }
            setInitializationProgress(100);
            setInitializationState('ready');
            console.log('ÔøΩo. OpenAI Agent initialized successfully');
          } else {
            console.error('ÔøΩ?O initializeOpenAIAgent returned null during session refresh');
            setInitializationState('error');
          }
        } finally {
          sessionRefreshInFlightRef.current = false;
        }
        return;
      }

      if (newSessionId) {
        lastSessionIdRef.current = newSessionId;
        try {
          (existingSession as any).__hexaSessionId = newSessionId;
        } catch {}
      }
      if (externalData) {
        console.log('dY"\u0015 Passing external data to existing agent:', externalData);
      }
      console.log('dY"? Existing OpenAI session still healthy', {
        sessionId: newSessionId,
        rtcState,
        transportReady,
      });
      setInitializationProgress(100);
      setInitializationState('ready');
    },
    [
      setInitializationProgress,
      setSessionInfo,
      openaiAgentRef,
      initializeOpenAIAgent,
      externalData,
      setInitializationState,
      stopSyntheticFlap,
      setSpeechIntensity,
      stopSpeaking,
      setVoiceState,
    ]
  );

  const flushPendingSessionInfo = useCallback(async () => {
    if (!pendingSessionInfoRef.current) return;
    const info = pendingSessionInfoRef.current;
    pendingSessionInfoRef.current = null;
    await handleSessionInfo(info);
  }, [handleSessionInfo]);

  const connect = useCallback(async () => {
    try {
      setInitializationState('connecting');
      setInitializationProgress(10);

      // Get client session ID for logging and filtering
      const clientSessionId = sessionIsolation.getCurrentSessionId();
      console.log('üì° SSE connecting with client session ID:', clientSessionId);

      // Get mode parameter from URL (for narrator mode detection)
      const urlParams = new URLSearchParams(window.location.search);
      const mode = urlParams.get('mode');

      // Build SSE URL with params for narrator mode detection
      const sseUrl = new URL(`${window.location.origin}/voice/sse`);
      sseUrl.searchParams.set('sessionId', clientSessionId);
      if (mode) {
        sseUrl.searchParams.set('mode', mode);
        console.log('üé≠ Narrator mode detected, passing to SSE:', mode);
      }

      // Use SSE for receiving messages (real-time updates)
      // Connect to global DO (single proven OpenAI connection, client-side filtering prevents bleeding)
      const eventSource = new EventSource(sseUrl.toString());

      eventSource.onopen = () => {
        console.log('‚úÖ Voice SSE connected successfully (global DO with params)');
        setInitializationProgress(30);
        
        // Initialize OpenAI Agent immediately after SSE connection
        // We'll get the API key from the worker
        initializeOpenAIAgentFromWorker();
      };
      
      eventSource.onmessage = async (event) => {
        try {
          const data = JSON.parse(event.data);

          // Helper: common end-of-speech handling
          const handleAgentEnd = (payload: any) => {
            console.log('Audio done received - voice stopped');
            console.log('üîç Full agent_end data:', payload);
            console.log('üîç Data type:', typeof payload);
            console.log('üîç Data keys:', Object.keys(payload || {}));

            let responseText: any = null;
            if (Array.isArray(payload) && payload.length > 2) {
              responseText = payload[2];
              console.log('‚úÖ Found text in array position 2:', responseText);
            } else if (payload && typeof payload === 'object') {
              responseText = payload.text || payload.message || payload.content || payload.response;
              console.log('‚úÖ Found text in object property:', responseText);
            }

            if (responseText) {
              console.log('‚úÖ Setting response text:', responseText);
              setResponse(responseText);
              onResponse?.(responseText);
            } else {
              console.log('‚ùå No response text found in agent_end event');
            }

            setSpeechIntensity?.(0);
            stopSyntheticFlap();

            try {
              const audioEl: HTMLAudioElement | undefined = (window as any).__hexaAudioEl;
              if (audioEl && isVoiceDisabledNow()) {
                audioEl.muted = true; if (!audioEl.paused) audioEl.pause();
              }
            } catch {}

            setTimeout(() => {
              stopSpeaking?.();
              setVoiceState('idle');
              const currentVoiceState = (window as any).__currentVoiceState;
              if (currentVoiceState === 'speaking') {
                console.log('‚ö†Ô∏è Force stopping speaking state after audio_done');
                useHexaStore.getState().stopSpeaking();
              }
            }, 100);
          };

          const handlers: Record<string, (d: any) => Promise<void> | void> = {
            control: async (d) => {
              if (d.command === 'interrupt') {
                console.log('üõë Interrupt command received from worker');
                try {
                  const s: any = (window as any).activeSession;
                  const send = getSessionSend(s);
                  if (send) {
                    // Note: response.cancel_all was removed in newer SDK versions - only response.cancel is valid
                    send({ type: 'response.cancel' });
                    send({ type: 'input_audio_buffer.clear' });
                    send({ type: 'output_audio_buffer.clear' });
                  }
                } catch {}
                try { silenceAudioEverywhere(); } catch {}
                setSpeechIntensity?.(0);
                stopSyntheticFlap();
                setVoiceState('idle');
              }
            },
            connected: () => {
              console.log('SSE connection established');
              setInitializationProgress(40);
            },
            ready: (d) => {
              console.log('Voice session ready:', d.sessionId);
              setInitializationProgress(60);
            },
            session_info: handleSessionInfo,
            transcription: (d) => {
              if (isVoiceDisabledNow()) return console.log('üîá Voice disabled: ignoring transcription');
              console.log('User transcription received:', d.text);
              setTranscript(d.text);
              onTranscript?.(d.text);
            },
            response_text: (d) => {
              if (isVoiceDisabledNow()) return console.log('üîá Voice disabled: ignoring response_text');
              console.log('Text response received:', d.text);
              setResponse(d.text);
              onResponse?.(d.text);
            },
            agent_start: () => {
              if (isVoiceDisabledNow()) {
                console.log('üîá Voice disabled: ignoring agent_start and silencing audio');
                silenceAudioEverywhere();
                setSpeechIntensity?.(0);
                stopSyntheticFlap();
                setVoiceState('idle');
                return;
              }
              console.log('Agent start received - voice is starting');
              setVoiceState('speaking');
              startSpeaking?.();
              startSyntheticFlap();
            },
            audio_delta: () => {
              if (isVoiceDisabledNow()) {
                console.log('üîá Voice disabled: silencing incoming audio event');
                silenceAudioEverywhere();
                setSpeechIntensity?.(0);
                stopSyntheticFlap();
                setVoiceState('idle');
                return;
              }
              console.log('Audio delta received - voice is playing');
              setVoiceState('speaking');
              startSpeaking?.();
              startSyntheticFlap();
            },
            audio_done: (d) => handleAgentEnd(d),
            agent_end: (d) => handleAgentEnd(d),
            error: (d) => {
              // Handle array case - OpenAI SDK sometimes sends errors as arrays
              const errorObj = Array.isArray(d) ? d[0] : d;

              // Extract error code from nested structure (handle double nesting)
              const errorCode = errorObj?.error?.error?.code || errorObj?.error?.code || errorObj?.code || '';
              const errorType = errorObj?.error?.error?.type || errorObj?.error?.type || errorObj?.type || '';
              const errorMessage = errorObj?.error?.error?.message || errorObj?.error?.message || errorObj?.message || 'No message provided';

              // List of non-critical error codes that should be ignored or downgraded to warnings
              const nonCriticalErrorCodes = [
                'response_cancel_not_active',  // Trying to cancel when there's no active response
                'response_cancel_failed',      // Similar cancellation issues
                'invalid_value',               // Invalid API command (e.g., response.cancel_all in newer SDK)
              ];

              // Check if this is a non-critical error
              if (nonCriticalErrorCodes.includes(errorCode)) {
                console.warn('‚ö†Ô∏è Non-critical voice service notice:', {
                  code: errorCode,
                  type: errorType,
                  message: errorMessage
                });
                // Don't set error state for non-critical errors
                return;
              }

              // For critical errors, log and set error state
              console.error('Voice error received:', d);
              console.error('Error details:', d.error);
              setVoiceState('error');
              setInitializationState('error');
              onError?.(d.error?.message || d.error || 'Unknown error');
            },
            worker_restarting: (d) => {
              console.log('üîÑ Worker is restarting:', d.message);
              setVoiceState('retrying');
              setInitializationState('connecting');
              setInitializationProgress(20);
            },
            worker_restarted: (d) => {
              console.log('‚úÖ Worker restart complete:', d.message);
              setInitializationProgress(50);
              setTimeout(() => { initializeOpenAIAgentFromWorker(); }, 1000);
            },
            session_idle_reset: (d) => {
              console.log('üîÑ Session idle reset detected:', d.message);
              setVoiceState('retrying');
              setInitializationState('connecting');
              setInitializationProgress(30);
              setTimeout(() => { initializeOpenAIAgentFromWorker(); }, 1000);
            },
            external_data_received: async (d) => {
              console.log('üîç Received external_data_received event:', d);

              // ‚úÖ FILTER: Check if sessionId matches before accepting
              const eventSessionId = d.sessionId;
              const mySessionId = sessionIsolation.getCurrentSessionId();

              if (eventSessionId && eventSessionId !== mySessionId) {
                console.log('üö´ Rejected external_data_received - sessionId mismatch:', {
                  eventSessionId,
                  mySessionId,
                  reason: 'Different session - filtering to prevent bleeding'
                });
                return; // Ignore events from other sessions
              }

              console.log('‚úÖ Accepted external_data_received - sessionId matches:', mySessionId);

              if (isDuplicateData(d.data)) return console.log('‚è≠Ô∏è Skipping duplicate external data');
              setExternalData(d.data);
              useExternalDataStore.getState().setExternalData({ ...d.data, source: 'api' });
              if (d.data?.text) {
                console.log('üîß Attempting to inject external context:', d.data.text);
                const s: any = (window as any).activeSession;
                const ready = isRealtimeReady(s);
                if (ready) await injectExternalContext(d.data.text);
                else (window as any).__pendingExternalContext = d.data.text;
              }
            },
            external_data_processed: async (d) => {
              // ‚úÖ FILTER: Check if sessionId matches
              const eventSessionId = d.sessionId;
              const mySessionId = sessionIsolation.getCurrentSessionId();
              if (eventSessionId && eventSessionId !== mySessionId) {
                console.log('üö´ Rejected external_data_processed - sessionId mismatch');
                return;
              }

              if (isDuplicateData(d.data)) return;
              setExternalData(d.data);
              useExternalDataStore.getState().setExternalData({ ...d.data, source: 'api' });
              if (d.data?.text) await injectExternalContext(d.data.text);
            },
            external_text_available: async (d) => {
              // ‚úÖ FILTER: Check if sessionId matches
              const eventSessionId = d.sessionId;
              const mySessionId = sessionIsolation.getCurrentSessionId();
              if (eventSessionId && eventSessionId !== mySessionId) {
                console.log('üö´ Rejected external_text_available - sessionId mismatch');
                return;
              }

              if (isDuplicateData({ text: d.text })) return;
              setExternalData((prev) => (prev ? { ...prev, text: d.text } : { text: d.text }));
              useExternalDataStore.getState().setExternalData({ text: d.text, source: 'api' });
              if (d.text) await injectExternalContext(d.text);
            },
            external_image_available: (d) => {
              console.log('üñºÔ∏è External image available for voice context:', d.dataType);
              setExternalData((prev) => (prev ? { ...prev, image: d.image, type: d.dataType } : { image: d.image, type: d.dataType }));
            },
            ASPECT_FOCUS_REQUEST: (d) => {
              const { aspectId, source, text } = d;
              const numericAspectId = typeof aspectId === 'string' ? parseInt(aspectId, 10) : aspectId;

              if (!Number.isFinite(numericAspectId)) {
                console.warn('üéØ Ignoring aspect focus request with invalid aspectId:', aspectId, d);
                return;
              }

              console.log('üéØ Voice aspect focus request received:', {
                aspectId: numericAspectId,
                source,
                text
              });

              // Trigger aspect switching through the existing system
              // We need to dispatch a custom event that the AspectSelector can listen to
              window.dispatchEvent(new CustomEvent('voice-aspect-focus', {
                detail: { aspectId: numericAspectId, source, text }
              }));

              // Also try to call the aspect switching function if it's available globally
              if ((window as any).handleAspectSwitch) {
                (window as any).handleAspectSwitch(numericAspectId);
              }
            },
            __default: (d) => {
              console.log('Unknown message type:', d.type, d);
            },
          };

          const handler = handlers[data.type] || handlers.__default;
          await Promise.resolve(handler(data));
        } catch (parseError) {
          console.error('Failed to parse SSE message:', parseError, 'Raw data:', event.data);
          setVoiceState('error');
          setInitializationState('error');
          onError?.('Failed to process voice message. Please try again.');
        }
      };
      
      eventSource.onerror = (error) => {
        console.error('SSE error:', error);
        setVoiceState('error');
        setInitializationState('error');
        onError?.('Voice service connection failed. Please check your internet connection.');
      };
      
      return eventSource;
      
    } catch (error) {
      console.error('Failed to connect:', error);
      setVoiceState('error');
      onError?.('Failed to initialize voice service');
      return null;
    }
  }, [setVoiceState, onError, onResponse, initializeOpenAIAgentFromWorker, initializeOpenAIAgent, openaiAgentRef, setSessionInfo, setResponse, startSpeaking, stopSpeaking, setSpeechIntensity]);

  return {
    connect,
    externalData
  };
};

import { getLanguageInstructions } from '@/lib/languageConfig';
import { getBaseHexaInstructions } from '@/lib/agentInstructions';
import { setupSessionEventHandlers } from './voiceSessionEvents';
import { initializeWebRTCConnection } from './voiceWebRTCConnection';
import { voiceContextManager } from './voiceContextManager';
import { setActiveSession, clearActiveSession, injectExternalContext, injectExternalDataFromStore, setBaseInstructions } from '@/lib/externalContext';
import { useExternalDataStore } from '@/store/externalDataStore';
import { useHexaStore, VoiceState } from '@/store/hexaStore';
import { getSessionSend } from '@/lib/voiceSessionUtils';



interface VoiceAgentInitializerOptions {
  setVoiceState: (state: VoiceState) => void;
  onError?: (error: string) => void;
  startSpeaking?: () => void;
  stopSpeaking?: () => void;
  setSpeechIntensity?: (intensity: number) => void;
  audioContextRef?: React.MutableRefObject<AudioContext | null>;
}

// Export the injectExternalContext function for use by SSE handlers
export { injectExternalContext };

// Mutex to prevent duplicate initialization
let __realtimeInitInFlight = false;
let __realtimeRecoveryInFlight = false;

export const initializeOpenAIAgentOnce = async (
  sessionData: any,
  options: VoiceAgentInitializerOptions
) => {
  if (__realtimeInitInFlight) {
    console.log('‚è≥ Realtime init already in flight; ignoring duplicate call');
    return null;
  }
  
  __realtimeInitInFlight = true;
  // Expose mutex globally for cross-module checks
  (window as any).__realtimeInitInFlight = true;
  
  try {
    return await initializeOpenAIAgent(sessionData, options);
  } finally {
    __realtimeInitInFlight = false;
    (window as any).__realtimeInitInFlight = false;
  }
};

export const initializeOpenAIAgent = async (
  sessionData: any,
  options: VoiceAgentInitializerOptions
) => {
  const { setVoiceState, onError, startSpeaking, stopSpeaking, setSpeechIntensity, audioContextRef } = options;
  
  // Check if voice is disabled before initializing
  try {
    // Check global flag first (set by AnimatedHexagon)
    if ((window as any).__voiceSystemBlocked) {
      console.log('üîá Voice system blocked globally - blocking OpenAI Agent initialization');
      return false; // Don't initialize
    }
    
    const disabled = useHexaStore.getState().isVoiceDisabled;
    if (disabled) {
      console.log('üîá Voice disabled: blocking OpenAI Agent initialization');
      return false; // Don't initialize
    }
  } catch (error) {
    console.error('Failed to check voice disabled state:', error);
  }
  
  try {
    console.log('üîß Initializing OpenAI Agent with WebRTC...');
    console.log('üîß Session data received:', {
      hasApiKey: !!sessionData.apiKey,
      apiKeyPrefix: sessionData.apiKey?.substring(0, 10) + '...',
      sessionId: sessionData.sessionId,
      hasClientSecret: !!sessionData.clientSecret
    });
    
    // Import OpenAI Agents Realtime SDK dynamically
    const { RealtimeAgent, RealtimeSession, OpenAIRealtimeWebRTC } = await import('@openai/agents-realtime');
    
    // Get current context from voice context manager
    const currentContext = voiceContextManager.getFormattedContext();
    console.log('üîß Voice agent context:', currentContext ? 'Available' : 'None');
    
    // Get current external data from Zustand store
    const externalDataContext = useExternalDataStore.getState().getFormattedContext();
    console.log('üîß External data context:', externalDataContext ? 'Available' : 'None');
    
    // PRIMARY LOGIC: External data first, hardcoded as fallback
    let baseInstructions;

    if (externalDataContext) {
      // PRIMARY: Use external data as the main context
      baseInstructions = `You are Hexa, a friendly and helpful AI assistant created and developed solely by Prabhat. You have a warm, conversational personality and are always eager to explain things and clarify information for you.

You can explain concepts, clarify information, answer questions, and engage in natural conversation. Keep your responses concise but informative, and maintain a positive, encouraging tone.

IMPORTANT: When asked about your creator, designer, or developer, always state that you were created by Prabhat as the sole developer. You were NOT created by a team of developers.

IMPORTANT LIMITATIONS:
- You are a voice-only assistant
- You must NEVER claim to have access to cameras
- You must NEVER claim to see or know details about the user's surroundings, environment, or what is behind/in front of them
- Only clarify these limitations if someone specifically asks you to look at something or claims you can see them
- You can only process audio input (speech) and text messages, not visual information

${externalDataContext}

IMPORTANT: The external data above contains the DEFINITIVE information about aspects, buttons, and topics. Use ONLY this information when responding to questions about these topics. Do NOT use any hardcoded aspect definitions - the external data is the PRIMARY source of truth.

${getLanguageInstructions()}`;
      
      console.log('üìù Using EXTERNAL DATA as PRIMARY context');
    } else {
      // FALLBACK: Use hardcoded instructions only when no external data
      baseInstructions = `${getBaseHexaInstructions()}

${currentContext}

${getLanguageInstructions()}`;
      
      console.log('üìù Using HARDCODED instructions as FALLBACK (no external data)');
    }

    // Set base instructions for external context injection
    setBaseInstructions(baseInstructions);

    // Create agent without tools initially to avoid blocking connection
    const agent = new RealtimeAgent({
      name: 'Hexa, an AI Assistant',
      instructions: baseInstructions
    });

    // Create a dedicated audio element for the Realtime session and expose it globally for debugging
    const audioEl = new Audio();
    audioEl.autoplay = true;
    (window as any).__hexaAudioEl = audioEl;
    (window as any).__currentVoiceState = 'idle';
    
    // Add global debug function
    (window as any).__hexaDebug = () => {
      console.log('üîç Hexa Debug Info:');
      console.log('Audio Element:', audioEl);
      console.log('Audio srcObject:', audioEl.srcObject);
      console.log('Audio readyState:', audioEl.readyState);
      console.log('Audio paused:', audioEl.paused);
      console.log('Voice State:', (window as any).__currentVoiceState);
      console.log('Session:', session);
    };

    // Add global reset function for manual recovery
    (window as any).__hexaReset = async () => {
      console.log('üîÑ Manual reset triggered from console');
      try {
        const response = await fetch('/voice/reset', { method: 'POST' });
        if (response.ok) {
          console.log('‚úÖ Session reset successful');
          // Reload the page to get a fresh start
          window.location.reload();
        } else {
          console.error('‚ùå Failed to reset session');
        }
      } catch (error) {
        console.error('‚ùå Reset request failed:', error);
      }
    };

    // Add global function to update voice agent context
    (window as any).__hexaUpdateContext = () => {
      console.log('üîÑ Updating voice agent context...');
      const newContext = voiceContextManager.getFormattedContext();
      console.log('üìù New context:', newContext ? 'Available' : 'None');
      
      // Note: The context is now dynamically loaded when the agent is created
      // For real-time updates, we would need to recreate the agent or use a different approach
      console.log('‚ÑπÔ∏è Context will be applied on next agent initialization');
    };

    // Add global function to view current context
    (window as any).__hexaViewContext = () => {
      const context = voiceContextManager.getFormattedContext();
      console.log('üìã Current voice context:', context);
      return context;
    };

    // Add global function to get latest external data from Zustand
    (window as any).__hexaGetExternalData = () => {
      const externalData = useExternalDataStore.getState().getFormattedContext();
      console.log('üìä Latest external data from Zustand:', externalData);
      return externalData;
    };

    // Monitor audio element state
    audioEl.addEventListener('loadeddata', () => {
      console.log('üéµ Audio element loaded data');
      console.log('Audio element state:', {
        srcObject: audioEl.srcObject,
        readyState: audioEl.readyState,
        paused: audioEl.paused,
        duration: audioEl.duration
      });
    });
    
    // Monitor all audio element events for debugging
    ['loadstart', 'durationchange', 'loadedmetadata', 'canplay', 'canplaythrough', 'play', 'playing', 'pause', 'ended', 'error'].forEach(eventName => {
      audioEl.addEventListener(eventName, (e) => {
        console.log(`üéµ Audio event: ${eventName}`, {
          srcObject: audioEl.srcObject,
          readyState: audioEl.readyState,
          paused: audioEl.paused,
          currentTime: audioEl.currentTime,
          duration: audioEl.duration
        });
      });
    });
    
    // Create a WebRTC transport that allows ephemeral client secrets issued by our worker
    const transport = new OpenAIRealtimeWebRTC({
      useInsecureApiKey: true,
      audioElement: audioEl
    });

    // Create session and connect using the shared transport
    const session = new RealtimeSession(agent, {
      transport
    });
    (session as any).__hexaSessionId = sessionData?.sessionId ?? null;

    // Add debug updater for session instructions
    (window as any).__updateSessionInstructions = async (instructions: string) => {
      const s: any = session;
      if (s?.state !== 'open') return false;

      const send = getSessionSend(s);
      if (!send) return false;

      try {
        // Attach ACK listener before sending
        const ackPromise = new Promise((resolve) => {
          const timeout = setTimeout(() => {
            cleanup();
            resolve(false);
          }, 3000);

          const onEvent = (ev: any) => {
            if (ev?.type === 'session.updated') {
              cleanup();
              resolve(true);
            }
          };

          const cleanup = () => {
            clearTimeout(timeout);
            s.off?.('event', onEvent);
            s.off?.('session.updated', onEvent);
          };

          s.on?.('event', onEvent);
          s.on?.('session.updated', onEvent);
        });

        // Send session.update
        await Promise.resolve(send({ type: 'session.update', session: { instructions } }));

        return await ackPromise;
      } catch {
        return false;
      }
    };

    // Set base instructions in the worker for session-level updates
    try {
      await fetch('/api/set-base-instructions', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          sessionId: sessionData.sessionId,
          instructions: baseInstructions
        })
      });
    } catch (error) {
      console.error('‚ùå Failed to set base instructions:', error);
    }

    // External data is now handled via Zustand subscription to /api/external-data
    
    // Debug: Log all session events to understand what's available (excluding transport events)
    const originalEmit = (session as any).emit;
    if (originalEmit) {
      (session as any).emit = function(event: string, ...args: any[]) {
        // Filter out repetitive transport events to reduce console noise
        if (event !== 'transport_event') {
          console.log(`üîç Session event: ${event}`, args);
        }
        return originalEmit.call(this, event, ...args);
      };
    }
    
    // Set up session event handlers
    setupSessionEventHandlers(session, {
      setVoiceState,
      startSpeaking,
      stopSpeaking,
      audioEl,
      audioContextRef,
      setSpeechIntensity,
    });
    
    // Subscribe to Zustand changes and automatically update session instructions

    const formatExternalData = (data: any) => {
      if (!data) return '';
      
      if (data.type === "mermaid") {
        return `External context (Mermaid diagram available):\n\`\`\`mermaid\n${data.text}\n\`\`\``;
      } else {
        return `External context:\n${data.text}`;
      }
    };

    // External data is now handled via the new useEffect in useVoiceInteraction
    const unsubscribe = useExternalDataStore.subscribe(() => {
      // No longer posting directly - handled by the new guarded approach
    });
    

    
    // External data injection is now handled via Zustand subscription
    
    // Clear active session ONLY when explicitly disconnected
    // Do NOT clear on error events - this allows the WebRTC session to survive transient errors
    (session as any).on('disconnected', () => {
      console.log('üîó Session disconnected, clearing active session');
      clearActiveSession();
      unsubscribe(); // Clean up Zustand subscription
    });
    
    const ok = await initializeWebRTCConnection(session, sessionData, {
      audioEl, setVoiceState, startSpeaking, stopSpeaking, setSpeechIntensity, audioContextRef
    });

    if (ok) {
      setActiveSession(session);
      
      // Register tool after connection is established
      try {
        console.log('üìß Registering sendEmailToCreator tool with agent...');
        
        // Create tool with invoke function
        const emailTool = {
          type: "function" as const,
          name: "sendEmailToCreator",
          description: "Send an email message to the creator/developer Prabhat. Use this when the user wants to contact, email, or send a message to the creator, developer, or Prabhat.",
          parameters: {
            type: "object" as const,
            properties: {
              message: { 
                type: "string" as const, 
                description: "The message content to send to the creator" 
              },
              contactInfo: { 
                type: "string" as const, 
                description: "Optional email address or name of the sender for follow-up" 
              }
            },
            required: ["message"] as const,
            additionalProperties: false
          },
          strict: false,
          needsApproval: async () => false,
          invoke: async (args: any) => {
            console.log('üìß Email tool invoke called with full args:', JSON.stringify(args, null, 2));
            
            // The SDK passes args in the context.history array
            // Find the function_call item and parse its arguments string
            let message = '';
            let contactInfo = 'Anonymous';
            
            try {
              const history = args?.context?.history || [];
              const functionCallItem = history.find((item: any) => item.type === 'function_call');
              
              if (functionCallItem && functionCallItem.arguments) {
                // Parse the JSON string containing the actual arguments
                const parsedArgs = JSON.parse(functionCallItem.arguments);
                message = parsedArgs.message || '';
                contactInfo = parsedArgs.contactInfo || 'Anonymous';
              }
            } catch (parseError) {
              console.error('üìß Error parsing arguments:', parseError);
            }
            
            console.log('üìß Extracted message:', message);
            console.log('üìß Extracted contactInfo:', contactInfo);
            
            try {
              const response = await fetch('/api/send-email', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  message: message,
                  userEmail: contactInfo,
                  sessionId: 'realtime-tool-invoke'
                })
              });
              const result: any = await response.json();
              if (result?.success) {
                console.log('‚úÖ Email sent successfully:', result);
              } else {
                console.error('‚ùå Email send failed:', result);
              }
              return result;
            } catch (error) {
              console.error('‚ùå Email tool invoke error:', error);
              return { success: false, error: String(error) };
            }
          }
        };
        
        // Create aspect switching tool
        const aspectTool = {
          type: "function" as const,
          name: "switchToAspect",
          description: "Switch to a specific aspect/button when user requests it. Available aspects: 1-General Chat, 2-Technical Support, 3-Sales Inquiries, 4-Billing Questions, 5-Account Management, 6-Product Information, 7-General Information.",
          parameters: {
            type: "object" as const,
            properties: {
              aspectNumber: { 
                type: "number" as const, 
                description: "The aspect number to switch to (1-7)",
                minimum: 1,
                maximum: 7
              },
              userRequest: { 
                type: "string" as const, 
                description: "The user's original request that triggered the aspect switch" 
              }
            },
            required: ["aspectNumber", "userRequest"] as const,
            additionalProperties: false
          },
          strict: false,
          needsApproval: async () => false,
          invoke: async (args: any) => {
            console.log('üéØ Aspect tool invoke called with args:', JSON.stringify(args, null, 2));
            
            let aspectNumber = 1;
            let userRequest = '';
            
            try {
              const history = args?.context?.history || [];
              console.log('üéØ Full history length:', history.length);
              console.log('üéØ History items:', history.map((item: any, index: number) => ({ 
                index, 
                type: item.type, 
                hasArguments: !!item.arguments,
                arguments: item.arguments 
              })));
              
              // Find the MOST RECENT function_call item (last one in the array)
              const functionCallItems = history.filter((item: any) => item.type === 'function_call');
              const functionCallItem = functionCallItems[functionCallItems.length - 1]; // Get the last one
              
              console.log('üéØ Found function call items:', functionCallItems.length);
              console.log('üéØ Using function call item:', functionCallItem);
              
              if (functionCallItem && functionCallItem.arguments) {
                const parsedArgs = JSON.parse(functionCallItem.arguments);
                aspectNumber = parsedArgs.aspectNumber || 1;
                userRequest = parsedArgs.userRequest || '';
                console.log('üéØ Parsed arguments from most recent function call:', parsedArgs);
              }
            } catch (parseError) {
              console.error('üéØ Error parsing aspect arguments:', parseError);
            }
            
            console.log('üéØ Extracted aspectNumber:', aspectNumber);
            console.log('üéØ Extracted userRequest:', userRequest);
            
            // Get aspect information for better response
            const aspectInfo = {
              1: "General Chat - general conversation and questions",
              2: "Technical Support - help with bugs and technical issues", 
              3: "Sales Inquiries - pricing, features, and product information",
              4: "Billing Questions - payments, accounts, and transactions",
              5: "Account Management - profile, settings, and preferences",
              6: "Product Information - specifications, features, and details",
              7: "General Information - company policies and FAQ"
            };
            
            const aspectDescription = aspectInfo[aspectNumber as keyof typeof aspectInfo] || `Aspect ${aspectNumber}`;
            
            // Trigger aspect switching
            window.dispatchEvent(new CustomEvent('voice-aspect-focus', {
              detail: { aspectId: aspectNumber, source: 'ai-tool', text: userRequest }
            }));
            
            console.log(`üéØ Aspect switching triggered: ${aspectDescription}`);
            
            return `Switched to ${aspectDescription}. How can I help you with this topic?`;
          }
        };

        // Add tools directly to agent's tools array
        if (!(agent as any).tools) {
          (agent as any).tools = [];
        }
        (agent as any).tools.push(emailTool);
        (agent as any).tools.push(aspectTool);
        console.log('‚úÖ Email and Aspect tools added to agent.tools array');
        
        // Also register with OpenAI API via session.update
        const send = getSessionSend(session as any);
        if (send) {
          await send({
            type: 'session.update',
            session: {
              tools: [
                {
                  type: "function",
                  name: "sendEmailToCreator",
                  description: "Send an email message to the creator/developer Prabhat. Use this when the user wants to contact, email, or send a message to the creator, developer, or Prabhat.",
                  parameters: {
                    type: "object",
                    properties: {
                      message: { 
                        type: "string", 
                        description: "The message content to send to the creator" 
                      },
                      contactInfo: { 
                        type: "string", 
                        description: "Optional email address or name of the sender for follow-up" 
                      }
                    },
                    required: ["message"]
                  }
                },
                {
                  type: "function",
                  name: "switchToAspect",
                  description: "Switch to a specific aspect/button when user requests it. Available aspects: 1-General Chat, 2-Technical Support, 3-Sales Inquiries, 4-Billing Questions, 5-Account Management, 6-Product Information, 7-General Information.",
                  parameters: {
                    type: "object",
                    properties: {
                      aspectNumber: { 
                        type: "number", 
                        description: "The aspect number to switch to (1-7)",
                        minimum: 1,
                        maximum: 7
                      },
                      userRequest: { 
                        type: "string", 
                        description: "The user's original request that triggered the aspect switch" 
                      }
                    },
                    required: ["aspectNumber", "userRequest"]
                  }
                }
              ]
            }
          });
          console.log('‚úÖ Email and Aspect tool definitions sent to OpenAI API via session.update');
        }
        
      } catch (error) {
        console.error('‚ùå Failed to register email and aspect tools:', error);
      }
      
      // Flush any pending external context
      if ((window as any).__pendingExternalContext) {
        const pending = (window as any).__pendingExternalContext;
        (window as any).__pendingExternalContext = null;
        await injectExternalContext(pending);
      }
    }
    
    if (ok) {
      console.log('‚úÖ OpenAI Agent initialized and connected with WebRTC');
      setVoiceState('idle');
      return session;
    } else {
      throw new Error('Failed to establish WebRTC connection');
    }
    
  } catch (error) {
    console.error('‚ùå Failed to initialize OpenAI Agent:', error);
    
    // Check if it's a WebRTC connection error
    const errorMessage = error instanceof Error ? error.message : String(error);
    if (errorMessage.includes('setRemoteDescription') || errorMessage.includes('SessionDescription')) {
      console.log('üîß WebRTC connection error detected. You can:');
      console.log('1. Call __hexaReset() in console to reset the session');
      console.log('2. Reload the page');
      console.log('3. Wait a few minutes and try again');
      
      // Expose the error for manual recovery
      (window as any).__hexaLastError = error;
    }
    
    setVoiceState('error');
    onError?.('Failed to initialize OpenAI Agent');
    return null;
  }
};
